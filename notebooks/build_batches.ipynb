{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BewwsBhUhC4"
   },
   "source": [
    "# Build train, validation and test dataset to train and evaluate MedGraphTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HATMy2KUoT_"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22222,
     "status": "ok",
     "timestamp": 1704654175458,
     "user": {
      "displayName": "Shira Ben David",
      "userId": "15174141733610170957"
     },
     "user_tz": -120
    },
    "id": "j2f4BGmZb30q",
    "outputId": "6e7b423e-e073-4833-ba95-31b97b78b21f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Thesis/MedTransNet\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/Thesis/MedTransNet\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Thesis/MedTransNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8287,
     "status": "ok",
     "timestamp": 1704654206902,
     "user": {
      "displayName": "Shira Ben David",
      "userId": "15174141733610170957"
     },
     "user_tz": -120
    },
    "id": "_lzVwU06aS7y",
    "outputId": "752d4382-4c75-41ff-d3d7-55f6340125b2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m15.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m19.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.3/115.3 kB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pyarrow-hotfix, dill, multiprocess, torch_geometric, datasets\n",
      "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 torch_geometric-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12228,
     "status": "ok",
     "timestamp": 1704654219126,
     "user": {
      "displayName": "Shira Ben David",
      "userId": "15174141733610170957"
     },
     "user_tz": -120
    },
    "id": "NGJZRgpEUEgK"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.utils import meta_relations_dict\n",
    "from config import ROOT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo5w3-DNUxi2"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSpbXMfHUyhN"
   },
   "outputs": [],
   "source": [
    "def convert_nx_to_hetero_data(graph: nx.Graph, edge_uid_offset=0) -> Tuple[HeteroData, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        graph: the nx.Graph from which the heteroData  should be created\n",
    "        edge_uid_offset: a pointer of the last added edge. Might be used across many transformed graph to keep track across batched/ datasets\n",
    "\n",
    "    Returns:\n",
    "        data: the HeteroData object created from the input graph\n",
    "        edge_uid_offset: the updated edge_uid_offset\n",
    "    \"\"\"\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    node_types_embeddings_dict = {}\n",
    "    node_types_uids_dict = {}\n",
    "    edge_types_index_dict = {}\n",
    "    edge_types_uids_dict = {}\n",
    "    answer_choice_order_list = []\n",
    "\n",
    "    # Iterate over all edges:\n",
    "    for index, (s, t, edge_attr) in enumerate(graph.edges(data=True)):\n",
    "\n",
    "        relation = meta_relations_dict[edge_attr['relation']]\n",
    "\n",
    "        s_node = graph.nodes[s]\n",
    "        s_node_type = s_node['type']\n",
    "        s_node_embedding = s_node['embedding']\n",
    "        s_uid = s_node['index']\n",
    "        s_choice_index = -1\n",
    "        if s_node_type == 'answer':\n",
    "            s_choice_index = s_node['answer_choice_index']\n",
    "\n",
    "        t_node = graph.nodes[t]\n",
    "        t_node_type = t_node['type']\n",
    "        t_node_embedding = t_node['embedding']\n",
    "        t_uid = t_node['index']\n",
    "        t_choice_index = -1\n",
    "        if t_node_type == 'answer':\n",
    "            t_choice_index = t_node['answer_choice_index']\n",
    "\n",
    "        if s_node_type != relation[0]:\n",
    "            s_node_type, t_node_type = t_node_type, s_node_type\n",
    "            s_node_embedding, t_node_embedding = t_node_embedding, s_node_embedding\n",
    "            s_uid, t_uid = t_uid, s_uid\n",
    "\n",
    "        if s_node_type not in node_types_embeddings_dict:\n",
    "            node_types_embeddings_dict[s_node_type] = []\n",
    "            node_types_uids_dict[s_node_type] = []\n",
    "            s_node_index = len(node_types_embeddings_dict[s_node_type])\n",
    "            node_types_embeddings_dict[s_node_type].append(s_node_embedding)\n",
    "            node_types_uids_dict[s_node_type].append(s_uid)\n",
    "            if s_choice_index != -1:\n",
    "                answer_choice_order_list.append(s_choice_index)\n",
    "\n",
    "        elif s_uid not in node_types_uids_dict[s_node_type]:\n",
    "            s_node_index = len(node_types_embeddings_dict[s_node_type])\n",
    "            node_types_embeddings_dict[s_node_type].append(s_node_embedding)\n",
    "            node_types_uids_dict[s_node_type].append(s_uid)\n",
    "            if s_choice_index != -1:\n",
    "                answer_choice_order_list.append(s_choice_index)\n",
    "\n",
    "        else:\n",
    "            s_node_index = node_types_uids_dict[s_node_type].index(s_uid)\n",
    "\n",
    "        if t_node_type not in node_types_embeddings_dict:\n",
    "            node_types_embeddings_dict[t_node_type] = []\n",
    "            node_types_uids_dict[t_node_type] = []\n",
    "            t_node_index = len(node_types_embeddings_dict[t_node_type])\n",
    "            node_types_embeddings_dict[t_node_type].append(t_node_embedding)\n",
    "            node_types_uids_dict[t_node_type].append(t_uid)\n",
    "            if t_choice_index != -1:\n",
    "                answer_choice_order_list.append(t_choice_index)\n",
    "\n",
    "        elif t_uid not in node_types_uids_dict[t_node_type]:\n",
    "            t_node_index = len(node_types_embeddings_dict[t_node_type])\n",
    "            node_types_embeddings_dict[t_node_type].append(t_node_embedding)\n",
    "            node_types_uids_dict[t_node_type].append(t_uid)\n",
    "            if t_choice_index != -1:\n",
    "                answer_choice_order_list.append(t_choice_index)\n",
    "\n",
    "        else:\n",
    "            t_node_index = node_types_uids_dict[t_node_type].index(t_uid)\n",
    "\n",
    "        if relation not in edge_types_index_dict:\n",
    "            edge_types_index_dict[relation] = []\n",
    "            edge_types_index_dict[relation].append([s_node_index, t_node_index])\n",
    "            edge_types_uids_dict[relation] = []\n",
    "            edge_types_uids_dict[relation].append(edge_uid_offset)\n",
    "            edge_uid_offset += 1\n",
    "\n",
    "        elif [s_node_index, t_node_index] not in edge_types_index_dict[relation]:\n",
    "            edge_types_index_dict[relation].append([s_node_index, t_node_index])\n",
    "            edge_types_uids_dict[relation].append(edge_uid_offset)\n",
    "            edge_uid_offset += 1\n",
    "\n",
    "    # Iterate over nodes with no neighbors:\n",
    "    nodes_with_no_neighbors = [graph.nodes[node] for node in graph.nodes() if len(list(graph.neighbors(node))) == 0]\n",
    "    for node in nodes_with_no_neighbors:\n",
    "        node_type = node['type']\n",
    "        node_embedding = node['embedding']\n",
    "        node_uid = node['index']\n",
    "        if node_embedding.dim() == 2:\n",
    "            node_embedding = torch.squeeze(node_embedding, dim=1)\n",
    "        if node_type not in node_types_embeddings_dict:\n",
    "            node_types_embeddings_dict[node_type] = []\n",
    "            node_types_uids_dict[node_type] = []\n",
    "            node_types_embeddings_dict[node_type].append(node_embedding)\n",
    "            node_types_uids_dict[node_type].append(node_uid)\n",
    "\n",
    "        elif node_uid not in node_types_uids_dict[node_type]:\n",
    "            node_types_embeddings_dict[node_type].append(node_embedding)\n",
    "            node_types_uids_dict[node_type].append(node_uid)\n",
    "\n",
    "    for n_type in node_types_embeddings_dict.keys():\n",
    "        x = torch.stack(node_types_embeddings_dict[n_type], dim=0).type(\"torch.FloatTensor\")\n",
    "        if x.dim() > 2:\n",
    "          x = x.squeeze(2)\n",
    "        data[n_type].x = x\n",
    "        data[n_type].node_uid = torch.tensor(node_types_uids_dict[n_type])\n",
    "        if n_type == 'answer':\n",
    "            data[n_type].answer_choices = torch.tensor(answer_choice_order_list)\n",
    "\n",
    "    for e_type in edge_types_index_dict.keys():\n",
    "        data[e_type].edge_index = torch.transpose(torch.tensor(edge_types_index_dict[e_type]), 0, 1)\n",
    "        data[e_type].edge_uid = torch.tensor(edge_types_uids_dict[e_type])\n",
    "\n",
    "    data = T.ToUndirected(merge=False)(data)\n",
    "\n",
    "    return data, edge_uid_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g12QtPV4YU4T"
   },
   "outputs": [],
   "source": [
    "def build_raw_data_list(root_dir: str):\n",
    "    data_list = []\n",
    "\n",
    "    print(f'Building raw data list...')\n",
    "\n",
    "    print(f'Building files names list...')\n",
    "    file_names_list = os.listdir(os.path.join(ROOT_DIR, root_dir))\n",
    "    print(f'Done')\n",
    "\n",
    "    for i, file_name in enumerate(file_names_list):\n",
    "        print(f'{i}/{len(file_names_list)}')\n",
    "        path = os.path.join(ROOT_DIR, root_dir, file_name)\n",
    "\n",
    "        graph_data_list = pickle.load(open(path, 'rb'))\n",
    "        for graph in graph_data_list:\n",
    "            data_list.append(graph)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPHh5gDEVOLZ"
   },
   "source": [
    "## Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKA6yXnSVLRJ"
   },
   "outputs": [],
   "source": [
    "class MedicalQADatasetBuilder:\n",
    "\n",
    "    def __init__(self,\n",
    "                 raw_data_list: List[nx.Graph],\n",
    "                 num_train_samples: int,\n",
    "                 processed_data_list: List[HeteroData] = None,\n",
    "                 positive_relation_type: Tuple[str, str, str] = ('question', 'question_correct_answer', 'answer'),\n",
    "                 neg_relation_type: Tuple[str, str, str] = ('question', 'question_wrong_answer', 'answer'),\n",
    "                 disjoint_train_edges_ratio: float = 0.9,\n",
    "                 negative_sampling_ratio: int = 3,\n",
    "                 batch_size: int = 32):\n",
    "\n",
    "        if processed_data_list is not None:\n",
    "            self.processed_data_list = processed_data_list\n",
    "\n",
    "        else:\n",
    "\n",
    "          self.raw_data_list = raw_data_list\n",
    "          self.processed_data_list = self.build_processed_data_list()\n",
    "\n",
    "        self.num_train_samples = num_train_samples\n",
    "        self.num_val_samples = (len(self.processed_data_list) - num_train_samples) // 2\n",
    "        self.num_test_samples = len(self.processed_data_list) - num_train_samples - self.num_val_samples\n",
    "\n",
    "        self.positive_relation_type = positive_relation_type\n",
    "        self.negative_relation_type = neg_relation_type\n",
    "        self.disjoint_train_ratio = disjoint_train_edges_ratio\n",
    "        self.negative_sampling_ratio = negative_sampling_ratio\n",
    "\n",
    "        self.processed_train_dataset = self.processed_data_list[:self.num_train_samples].copy()\n",
    "        self.processed_val_dataset = self.processed_data_list[: self.num_train_samples + self.num_val_samples].copy()\n",
    "        self.processed_test_dataset = self.processed_data_list.copy()\n",
    "\n",
    "        # shuffle val and test datasets\n",
    "        val_indices = list(range(len(self.processed_val_dataset)))\n",
    "        test_indices = list(range(len(self.processed_test_dataset)))\n",
    "\n",
    "        # Shuffle the original lists along with their index lists\n",
    "        combined_val = list(zip(self.processed_val_dataset, val_indices))\n",
    "        random.shuffle(combined_val)\n",
    "        self.processed_val_dataset, val_indices_shuffled = zip(*combined_val)\n",
    "\n",
    "        combined_test = list(zip(self.processed_test_dataset, test_indices))\n",
    "        random.shuffle(combined_test)\n",
    "        self.processed_test_dataset, test_indices_shuffled = zip(*combined_test)\n",
    "\n",
    "        self.train_loader = DataLoader(self.processed_train_dataset, batch_size=batch_size)\n",
    "        self.train_mini_batches = self.preprocess_batches(self.train_loader)\n",
    "        self.train_edges_dict = self.find_edges_split(self.train_mini_batches)\n",
    "        pickle.dump(self.train_mini_batches, open(os.path.join(ROOT_DIR, 'datasets', 'train', f'train_mini_batches_{batch_size}.pickle'), 'wb'))\n",
    "\n",
    "        self.val_loader = DataLoader(self.processed_val_dataset, batch_size=batch_size)\n",
    "        self.val_mini_batches = self.preprocess_batches(self.val_loader, is_train=False, edge_index_uids_dict=self.train_edges_dict)\n",
    "        self.val_edge_dict = self.find_edges_split(self.val_mini_batches)\n",
    "        pickle.dump(self.val_mini_batches, open(os.path.join(ROOT_DIR, 'datasets', 'validation', f'val_mini_batches_{batch_size}.pickle'), 'wb'))\n",
    "\n",
    "        self.test_loader = DataLoader(self.processed_test_dataset, batch_size=batch_size)\n",
    "        self.test_mini_batches = self.preprocess_batches(self.test_loader, is_train=False, edge_index_uids_dict=self.val_edge_dict)\n",
    "        pickle.dump(self.test_mini_batches, open(os.path.join(ROOT_DIR, 'datasets', f'test_mini_batches_{batch_size}.pickle'), 'wb'))\n",
    "\n",
    "    def build_processed_data_list(self):\n",
    "\n",
    "        processed_data_list = []\n",
    "        edge_uid_offset = 0\n",
    "\n",
    "        print('creating hetero data...')\n",
    "        for graph in tqdm(self.raw_data_list):\n",
    "            hetero_data, edge_uid_offset = convert_nx_to_hetero_data(graph, edge_uid_offset=edge_uid_offset)\n",
    "            if 'node_uid'not in hetero_data['question']:\n",
    "                continue\n",
    "            processed_data_list.append(hetero_data)\n",
    "\n",
    "        return processed_data_list\n",
    "\n",
    "    def shuffle_dataset(self, dataset_list: List[HeteroData]):\n",
    "        random.shuffle(dataset_list)\n",
    "        return dataset_list\n",
    "\n",
    "    def preprocess_batches(self, data_loader: DataLoader, is_train=True, edge_index_uids_dict=None):\n",
    "\n",
    "        processed_batches = []\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "\n",
    "            batch = self.ensure_batch_uniqueness(batch)\n",
    "            if is_train:\n",
    "\n",
    "                \"\"\"\n",
    "                The train dataset is the first one being processed --> no batch have been  used in other datasets.\n",
    "                \"\"\"\n",
    "\n",
    "                num_positive_edges = batch[self.positive_relation_type].edge_index.size(1)\n",
    "\n",
    "                positive_perm = torch.randperm(num_positive_edges)\n",
    "\n",
    "                num_message_passing_edges = int(self.disjoint_train_ratio * num_positive_edges)\n",
    "\n",
    "                positive_edge_index_indices = positive_perm[num_message_passing_edges:]\n",
    "                positive_edge_label_index_indices = positive_perm[:num_message_passing_edges]\n",
    "\n",
    "                # Find EdgeStore attributed for positive_relation_type\n",
    "                positive_edge_index = batch[self.positive_relation_type].edge_index[:, positive_edge_index_indices]\n",
    "                positive_edge_label_index = batch[self.positive_relation_type].edge_index[:, positive_edge_label_index_indices]\n",
    "                positive_edge_label = torch.ones((1, positive_edge_label_index.size(1)))\n",
    "\n",
    "            else:\n",
    "                positive_edge_index_indices, positive_edge_label_index_indices = self.split_labels(batch, edge_index_uids_dict)\n",
    "\n",
    "                if positive_edge_index_indices is None:\n",
    "                    continue  # todo: find a better way to handle imbalanced batches - where all edges have been seen -> reshuffle?\n",
    "\n",
    "                positive_edge_index = batch[self.positive_relation_type].edge_index[:, positive_edge_index_indices]\n",
    "                positive_edge_label_index = batch[self.positive_relation_type].edge_index[:, positive_edge_label_index_indices]\n",
    "                labels_length = 1 if positive_edge_label_index_indices.dim() == 0 else len(positive_edge_label_index_indices)\n",
    "                positive_edge_label = torch.ones((1, labels_length))\n",
    "\n",
    "            positive_edge_index_uids = batch[self.positive_relation_type].edge_uid[positive_edge_index_indices]\n",
    "            positive_edge_label_uids = batch[self.positive_relation_type].edge_uid[positive_edge_label_index_indices]\n",
    "\n",
    "            # Find the EdgeStore attributes for positive_relation_type (self.negative_ampler ensures each batch contain all answer possibilities per question)\n",
    "            negative_edge_index, negative_edge_index_uids = self.negative_sampler(batch, positive_edge_index[0])\n",
    "            negative_edge_label_index, negative_edge_label_uids = self.negative_sampler(batch, positive_edge_label_index[0])\n",
    "            negative_edge_label = torch.zeros((1, negative_edge_label_index.size(1)))\n",
    "\n",
    "            # Set EdgeStore attribute\n",
    "            batch[self.positive_relation_type].edge_index = positive_edge_index\n",
    "            batch[self.positive_relation_type].edge_label_index = positive_edge_label_index\n",
    "            batch[self.positive_relation_type].edge_label = positive_edge_label\n",
    "            batch[self.positive_relation_type].edge_index_uid = positive_edge_index_uids\n",
    "            batch[self.positive_relation_type].edge_label_uid = positive_edge_label_uids\n",
    "\n",
    "            batch[self.negative_relation_type].edge_index = negative_edge_index\n",
    "            batch[self.negative_relation_type].edge_label_index = negative_edge_label_index\n",
    "            batch[self.negative_relation_type].edge_label = negative_edge_label\n",
    "            batch[self.negative_relation_type].edge_index_uid = negative_edge_index_uids\n",
    "            batch[self.negative_relation_type].edge_label_uid = negative_edge_label_uids\n",
    "\n",
    "            # Set EdgeStore attributes for the reverse relations\n",
    "            rev_positive_relation_type = (self.positive_relation_type[2], f'rev_{self.positive_relation_type[1]}', self.positive_relation_type[0])\n",
    "            rev_negative_relation_type = (self.negative_relation_type[2], f'rev_{self.negative_relation_type[1]}', self.negative_relation_type[0])\n",
    "\n",
    "            batch[rev_positive_relation_type].edge_index = positive_edge_index.flip([0])\n",
    "            batch[rev_positive_relation_type].edge_label_index = positive_edge_label_index.flip([0])\n",
    "            batch[rev_positive_relation_type].edge_label = positive_edge_label\n",
    "            batch[rev_positive_relation_type].edge_index_uid = positive_edge_index_uids\n",
    "            batch[rev_positive_relation_type].edge_label_uid = positive_edge_label_uids\n",
    "\n",
    "            batch[rev_negative_relation_type].edge_index = negative_edge_index.flip([0])\n",
    "            batch[rev_negative_relation_type].edge_label_index = negative_edge_label_index.flip([0])\n",
    "            batch[rev_negative_relation_type].edge_label = negative_edge_label\n",
    "            batch[rev_negative_relation_type].edge_index_uid = negative_edge_index_uids\n",
    "            batch[rev_negative_relation_type].edge_label_uid = negative_edge_label_uids\n",
    "\n",
    "            processed_batches.append(batch)\n",
    "\n",
    "        return processed_batches\n",
    "\n",
    "    def negative_sampler(self, batch, source_node_indices):\n",
    "        negative_examples = []\n",
    "        negative_edge_uids = []\n",
    "        negative_indices = batch[self.negative_relation_type].edge_index\n",
    "\n",
    "        if source_node_indices.dim() == 0:\n",
    "            source_node_indices = source_node_indices.unsqueeze(0)\n",
    "\n",
    "        for index in source_node_indices:\n",
    "            negative_example_indices = torch.where(negative_indices[0] == index)[0][:self.negative_sampling_ratio]\n",
    "            negative_examples.append(negative_indices[:, negative_example_indices])\n",
    "            negative_edge_uids.append(batch[self.negative_relation_type].edge_uid[negative_example_indices])\n",
    "\n",
    "        return torch.cat(negative_examples, dim=1), torch.cat(negative_edge_uids)\n",
    "\n",
    "    def split_labels(self, batch, edge_index_uids_dict):\n",
    "        \n",
    "        # Extract edge_uid for positive_relation_type\n",
    "        edge_uids = batch[self.positive_relation_type].edge_uid\n",
    "\n",
    "        # Convert edge_index_uids_dict values to a tensor\n",
    "        uids_to_find = torch.tensor(list(edge_index_uids_dict[self.positive_relation_type]), dtype=edge_uids.dtype, device=edge_uids.device)\n",
    "\n",
    "        # Create a mask indicating where the uids are found in edge_uids\n",
    "        mask = torch.isin(edge_uids, uids_to_find)\n",
    "\n",
    "        # If all edge_uids are found, return None, None\n",
    "        if mask.all():\n",
    "            return None, None\n",
    "\n",
    "        # Find indices where edge_uids are not in uids_to_find\n",
    "        edge_index_indices = torch.where(mask)[0]\n",
    "        edge_label_index_indices = torch.where(~mask)[0]\n",
    "\n",
    "        return edge_index_indices, edge_label_index_indices\n",
    "\n",
    "    def find_edges_split(self, batches_list):\n",
    "        # Create edge_index_dict\n",
    "        edge_index_uids_dict = {}\n",
    "\n",
    "        for batch in batches_list:\n",
    "            for edge_type in batch.edge_types:\n",
    "\n",
    "                if edge_type not in edge_index_uids_dict:\n",
    "                    edge_index_uids_dict[edge_type] = []\n",
    "\n",
    "                edge_index_uids_dict[edge_type].append(batch[edge_type].edge_uid)\n",
    "\n",
    "        for edge_type in edge_index_uids_dict.keys():\n",
    "            edge_index_uids_dict[edge_type] = torch.cat(edge_index_uids_dict[edge_type], dim=0)\n",
    "\n",
    "        return edge_index_uids_dict\n",
    "\n",
    "    def ensure_batch_uniqueness(self, batch):\n",
    "        for node_type in batch.node_types:\n",
    "            edges_dict = {}\n",
    "\n",
    "            for edge_type in batch.edge_types:\n",
    "\n",
    "                if edge_type[0] == node_type:\n",
    "                    edges_dict[edge_type] = 0\n",
    "                elif edge_type[2] == node_type:\n",
    "                    edges_dict[edge_type] = 1\n",
    "\n",
    "            unique_node_type_features, unique_indices = torch.unique(batch[node_type].x, dim=0, return_inverse=True)\n",
    "\n",
    "            if unique_node_type_features.size(1) == batch[node_type].x.size(1):\n",
    "                continue\n",
    "\n",
    "            batch[node_type].x = unique_node_type_features\n",
    "\n",
    "            for edge_type, index in edges_dict.items():\n",
    "                for j in range(batch[edge_type].edge_index.size(1)):\n",
    "                    batch[edge_type].edge_index[index, j] = unique_indices[batch[edge_type].edge_index[index, j]]\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2L4fZ1uW-nj"
   },
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "raw_train_data_lists_path = os.path.join(ROOT_DIR, 'dataset', 'raw_train_data_lists')\n",
    "raw_train_data_list = build_raw_data_list(raw_train_data_lists_path)"
   ],
   "metadata": {
    "id": "fMQcJMTJJ6YS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4-N7QB5VNFL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1704035075902,
     "user_tz": -120,
     "elapsed": 6215,
     "user": {
      "displayName": "Shira Ben David",
      "userId": "15174141733610170957"
     }
    },
    "outputId": "3e713f40-0701-4276-a1fd-cc088e996d0a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 127/127 [00:01<00:00, 69.78it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_builder = MedicalQADatasetBuilder(\n",
    "            raw_train_data_list,\n",
    "            disjoint_train_edges_ratio=0.8,\n",
    "            num_train_samples=95000,\n",
    "            negative_sampling_ratio=3,\n",
    "            batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save, process or examine the created batches "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "authorship_tag": "ABX9TyP6FUu33EP3QJYvR4qKe+Dn"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
